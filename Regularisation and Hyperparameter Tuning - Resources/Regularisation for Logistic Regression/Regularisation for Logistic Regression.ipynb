{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bb15be5f-0ce5-49c2-818d-bed8697248b2",
   "metadata": {},
   "source": [
    "## Regularisation for Logistic Regression\n",
    "\n",
    "We will demonstrate how **regularisation** can help address overfitting in binary logistic regression using a dataset of network information, where the goal is to predict whether a network attack is detected\n",
    "\n",
    "In addition, we will carry out **hyperparameter tuning** to identify the optimal regularisation strength"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d04ff36-e964-472e-ac20-5666b96b1304",
   "metadata": {},
   "source": [
    "Let's start by importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80a9380-d84f-4750-a1ae-c769ce846406",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression  # Logistic regression model\n",
    "from sklearn.preprocessing import MinMaxScaler  # Scaling\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV  # Train-test split and grid search for hyperparameter tuning\n",
    "from sklearn.metrics import accuracy_score  # Classification performance\n",
    "import numpy as np; import pandas as pd; import matplotlib.pyplot as plt; import seaborn as sns  # Data processing and visualisation\n",
    "import warnings; warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3cce43f-d9b5-44e1-bcb3-d830c1d8bb30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('network_data.csv', index_col = 'session_id').iloc[:99, :]; df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d86059-422a-4725-9398-d9c60d1d5f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2694d0dc-4b9b-4df3-aba1-807082e42262",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['encryption_used'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a31d97-df37-418d-a004-773c5573bf8d",
   "metadata": {},
   "source": [
    "When there's no encryption used (i.e., `'encryption_used'` contains null values), we can consider it as a category of its own; so let's set this to the string `'None'`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b8c8ac2-ab07-4db1-a8f1-7e65df791cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['encryption_used'] = df['encryption_used'].fillna('None')  # Assume null values mean there is no encryption"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e653284b-ca24-46fb-b048-0ca834161b6f",
   "metadata": {},
   "source": [
    "We can see that our data contains a mix of numerical and categorical predictors; let's perform one-hot encoding on the categorical predictors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c6cf85-786b-4201-b0a8-b645eed93233",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_dummies = pd.get_dummies(df, drop_first = True)  # One-hot encoding\n",
    "df_with_dummies.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4165651a-44c1-4f55-87f2-66a8bf917696",
   "metadata": {},
   "source": [
    "Let's proceed by performing a train-test split on our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e8fea7-68a5-4086-a282-57dc0f98d027",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_with_dummies.drop('attack_detected', axis = 1); y = df_with_dummies['attack_detected']  # Predictors and target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)  # Train-test split\n",
    "print('Dimensions of X_train:', X_train.shape); print('Dimensions of y_train:', y_train.shape)\n",
    "print('Dimensions of X_test:', X_test.shape); print('Dimensions of y_test:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecec83db-982e-40f1-b990-0fe0d6aee0a2",
   "metadata": {},
   "source": [
    "Let's now check to see if our numerical predictors are on the same scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46899606-6239-4057-abcf-f4444a3dfd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8b2936-3ba2-40ec-9b8c-b8ee6aa26e6e",
   "metadata": {},
   "source": [
    "We can see here that our predictors are not on the same scale\n",
    "\n",
    "In general, and **especially while regularising our data**, it is important that our predictors be on the same scale; we will use min-max scaling to achieve this\n",
    "\n",
    "Remember, you should fit the scaler only on the training data and then apply that fitted transformation to the testing data. This ensures the testing set remains unseen and unbiased, while still being scaled consistently with the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e3f8ff-cbf1-4a78-b687-4f2def7287c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4850cf0e-80ae-47e7-8ec7-49dbf4fa9f18",
   "metadata": {},
   "source": [
    "Let's now fit our data to a logistic regression model and evaluate the accuracy of its predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aceca33-b6f2-476a-a339-e1e57f207c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(); logreg_model.fit(X_train, y_train)  # Fitting logistic regression model on training data\n",
    "y_pred_train = logreg_model.predict(X_train); y_pred_test = logreg_model.predict(X_test)  # Training and testing predictions\n",
    "acc_train = accuracy_score(y_train, y_pred_train); acc_test = accuracy_score(y_test, y_pred_test)  # Training and testing accuracies\n",
    "print('Training accuracy =', np.round(acc_train, 2)); print('Testing accuracy =', np.round(acc_test, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6920848-b55c-44f8-a590-dc2da444bcf5",
   "metadata": {},
   "source": [
    "### Lasso (L1) Regularisation\n",
    "\n",
    "Let's now apply lasso regularisation using the `'liblinear'` solver; this solver supports L1 regularisation\n",
    "\n",
    "The parameter `C` is the inverse of regularisation strength $\\alpha$, i.e., a larger `C` means we are trusting our data more. We will start with a `C = 100`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5dda874-b2ec-4c3a-8c03-7e29aa20043f",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(penalty = 'l1', solver = 'liblinear', C = 100); logreg_model.fit(X_train, y_train)  # Model fit\n",
    "y_pred_train = logreg_model.predict(X_train); y_pred_test = logreg_model.predict(X_test)  # Predictions\n",
    "acc_train = accuracy_score(y_train, y_pred_train); acc_test = accuracy_score(y_test, y_pred_test)  # Accuracies\n",
    "print('Training accuracy =', np.round(acc_train, 2)); print('Testing accuracy =', np.round(acc_test, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584184e2-23a3-463d-beab-adecf365eff1",
   "metadata": {},
   "source": [
    "`C` is a **hyperparameter** as it is set before training begins. Let's try to find an ideal `C` value by performing hyperparameter tuning using `GridSearchCV()` from `sklearn.model_selection`. This function will search a 'grid' of parameters, which is essentially an array of different `C` values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c43713-42c6-422a-b411-fb299022dff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter grid\n",
    "param_grid = {'C': np.logspace(-3, 3, num = 7)}  # from 0.001 to 1000 (logarithmic)\n",
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00fd781-13f6-4ea3-9eef-b16e8f0afbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(penalty = 'l1', solver = 'liblinear', max_iter = 1000)  # Setting a maximum number of iterations to prevent infinite search\n",
    "\n",
    "# GridSearch with CV\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv = 5, scoring = 'accuracy', return_train_score = True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract results\n",
    "results = grid_search.cv_results_\n",
    "C_values = results['param_C'].data.astype(float)\n",
    "mean_test = results['mean_test_score']\n",
    "mean_train = results['mean_train_score']\n",
    "\n",
    "# Plot with a logarithmic X-axis\n",
    "plt.figure(figsize = (6, 4))\n",
    "plt.semilogx(C_values, mean_train, marker = 'o', label = 'Training Accuracy', color = 'orange')\n",
    "plt.semilogx(C_values, mean_test, marker = 'o', label = 'Validation Accuracy', color = 'blue')\n",
    "\n",
    "plt.suptitle('Lasso (L1) Regularisation')\n",
    "plt.title('Accuracy vs Inverse of Regularisation Strength (C)'); plt.xlabel('C (log scale)'); plt.ylabel('Accuracy'); plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a87fe9f-84b3-4c0f-aee2-fe55221989c3",
   "metadata": {},
   "source": [
    "Choosing the right `C` value is ultimately a judgement call for the data scientist. It involves balancing the trade-off between fitting the training data well and achieving good validation performance, since too large or too small a value can either overfit or underfit the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d398dc-553a-4095-b502-6bd77b494be9",
   "metadata": {},
   "source": [
    "### Ridge (L2) Regularisation  \n",
    "\n",
    "Letâ€™s now apply ridge regularisation using the `'liblinear'` solver; this solver also supports L2 regularisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "add67150-832a-406c-942b-c5a59040ce08",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_model = LogisticRegression(penalty = 'l2', solver = 'liblinear', C = 100); logreg_model.fit(X_train, y_train)  # Model fit\n",
    "y_pred_train = logreg_model.predict(X_train); y_pred_test = logreg_model.predict(X_test)  # Predictions\n",
    "acc_train = accuracy_score(y_train, y_pred_train); acc_test = accuracy_score(y_test, y_pred_test)  # Accuracies\n",
    "print('Training accuracy =', np.round(acc_train, 2)); print('Testing accuracy =', np.round(acc_test, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b17083-d7cb-403c-8080-f011c433a9ed",
   "metadata": {},
   "source": [
    "We will use the same parameter grid for `C` as before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bb9c99-4e63-4e95-b07f-71daff1d9a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee10434-08ff-45cb-a5a3-536446a0835c",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(penalty = 'l2', solver = 'liblinear', max_iter = 1000)  # Setting a maximum number of iterations to prevent infinite search\n",
    "\n",
    "# GridSearch with CV\n",
    "grid_search = GridSearchCV(logreg, param_grid, cv = 5, scoring = 'accuracy', return_train_score = True)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Extract results\n",
    "results = grid_search.cv_results_\n",
    "C_values = results['param_C'].data.astype(float)\n",
    "mean_test = results['mean_test_score']\n",
    "mean_train = results['mean_train_score']\n",
    "\n",
    "# Plot with a logarithmic X-axis\n",
    "plt.figure(figsize = (6, 4))\n",
    "plt.semilogx(C_values, mean_train, marker = 'o', label = 'Training Accuracy', color = 'orange')\n",
    "plt.semilogx(C_values, mean_test, marker = 'o', label = 'Validation Accuracy', color = 'blue')\n",
    "\n",
    "plt.suptitle('Ridge (L2) Regularisation')\n",
    "plt.title('Accuracy vs Inverse of Regularisation Strength (C)'); plt.xlabel('C (log scale)'); plt.ylabel('Accuracy'); plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21170cea-36db-4819-aab7-947af118885c",
   "metadata": {},
   "source": [
    "Once again, the final choice of `C` depends on you as the data scientist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd22ddd-8e4a-4ef2-94b1-c5f8db85c616",
   "metadata": {},
   "source": [
    "### Comparison\n",
    "\n",
    "Let's compare how our model coefficients change depending on the kind of regularisation; let's go for `C = 1` in both cases here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a55a63f-f880-4937-8fbe-06441cac9248",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit L1 and L2 models\n",
    "logreg_l1 = LogisticRegression(penalty = 'l1', solver = 'liblinear', C = 1, max_iter = 1000).fit(X_train, y_train)\n",
    "logreg_l2 = LogisticRegression(penalty = 'l2', solver = 'liblinear', C = 1, max_iter = 1000).fit(X_train, y_train)\n",
    "\n",
    "# Coefficients\n",
    "coef_l1 = logreg_l1.coef_.flatten()\n",
    "coef_l2 = logreg_l2.coef_.flatten()\n",
    "\n",
    "# Plot coefficient values against feature index\n",
    "plt.figure(figsize = (6, 4))\n",
    "plt.plot(np.sort(coef_l1), marker = 'o', label = 'L1 (Lasso)')\n",
    "plt.plot(np.sort(coef_l2), marker = 'x', label = 'L2 (Ridge)')\n",
    "\n",
    "plt.title('Coefficient Magnitudes: L1 vs L2 Regularisation'); plt.xlabel('Feature Index (sorted)'); plt.ylabel('Coefficient Value'); plt.legend()\n",
    "plt.grid(True, linestyle = '--', alpha = 0.7);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cee1478-6f44-431a-9dc7-aa8d6330acaa",
   "metadata": {},
   "source": [
    "You can observe how lasso regression leads to harsher edges, while ridge regression tends to be smooth\n",
    "\n",
    "Here are the differences between these two regularisation techniques\n",
    "\n",
    "| Aspect                  | L1 Regularisation (Lasso)              | L2 Regularisation (Ridge)           |\n",
    "|--------------------------|-----------------------------------------|--------------------------------------|\n",
    "| Penalty Term            | Sum of absolute values of coefficients | Sum of squared values of coefficients |\n",
    "| Effect on Coefficients  | Forces some coefficients to exactly 0 (feature selection) | Shrinks coefficients but rarely makes them 0 |\n",
    "| Model Interpretation    | Produces sparse models, easier to interpret | Retains all features, less sparse |\n",
    "| When Useful             | High-dimensional data, need feature selection | Multicollinearity, need stability |\n",
    "| Solver Support (`sklearn`)| `'liblinear'`, `'saga'`                 | Most solvers (`'liblinear'`, `'lbfgs'`, `'saga'`, etc.) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

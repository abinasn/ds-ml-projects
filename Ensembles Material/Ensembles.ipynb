{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xdKzDBtRUXSy"
   },
   "source": [
    "# Part I - Introduction and Data Description\n",
    "\n",
    "For this demonstration, we will use a bank marketing data set. A bank ran a marketing campaign in the past and has obtained data pertaining to nearly 45,000 customers, which includes variables such as their age, jobs, bank balance, education, loan status and so on. Based on this data, the bank wants to develop its future strategies based on the insights that it drew from the previous campaign and improve for the next campaign so that more customers agree to open term deposits with the bank.\n",
    "\n",
    "Here, `'y'` (whether the customer wishes to open a deposit or not) is the target variable. A `'Yes'` in the `'y'` column indicates that the campaign was successful and the customer agreed to open a term deposit account with the bank. In contrast, a `'No'` in the `'y'` column indicates that the campaign was not very successful and the customer could not be convinced to open a term deposit account.\n",
    "\n",
    "The purpose of this demonstration is to show the viewer how to build and implement random forest models and gradient boosted tree models for classification. We will also look at how model performance varies for different values of various hyperparameters.\n",
    "\n",
    "## Data description\n",
    "\n",
    "### Input features\n",
    "- ***age*** : Age of the customer (numeric)\n",
    "- ***job*** : Type of job (categorical)\n",
    "- ***marital*** : Marital status (categorical)\n",
    "- ***education***: Level of education (categorical)\n",
    "- ***default***: Does the customer have a credit default or not? (categorical: 'no', 'yes', 'unknown')\n",
    "- ***balance***: Bank balance of the customer (numeric)\n",
    "- ***housing***: Does the customer have a housing loan or not? (categorical: 'no', 'yes', 'unknown')\n",
    "- ***loan***: Does the customer have a personal loan or not? (categorical: 'no', 'yes', 'unknown')\n",
    "- ***contact***: Contact communication type (categorical)\n",
    "- ***day***: Last contact day of the week (categorical)\n",
    "- ***month***: Last contact month of year (categorical)\n",
    "- ***duration***: Last contact duration, in seconds (numeric)\n",
    "- ***campaign***: Number of contacts performed during this campaign and for this client (numeric, includes last contact)\n",
    "- ***pdays***: Number of days that passed by after the client was last contacted from a previous campaign (numeric)\n",
    "- ***previous***: Number of contacts performed before this campaign and for this client (numeric)\n",
    "- ***poutcome***: Outcome of the previous marketing campaign (categorical)\n",
    "\n",
    "### Output feature\n",
    "- ***y***: Has the client subscribed a term deposit? (categorical: 'yes', 'no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install lightgbm  # Run this cell to install the lightgbm library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ViPVs4sOxQQ"
   },
   "outputs": [],
   "source": [
    "# Importing 'numpy' and 'pandas' packages for working with numbers and data frames\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Importing 'matplotlib.pyplot' and 'seaborn' for visualisations\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Importing packages for building ensemble models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier \n",
    "\n",
    "# Importing method for train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Importing sutiable error measure methods\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, roc_auc_score\n",
    "\n",
    "# Import 'GridSearchCV' for hyperparameter tuning\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Ignore warnings to keep output clean\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ub7eLnfcO_wa"
   },
   "outputs": [],
   "source": [
    "# Importing the raw data\n",
    "df = pd.read_csv('bank-full.csv', delimiter = ';')  # This dataset uses semicolons instead of commas to separate values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "_ewY3GSgTh0s",
    "outputId": "396efc95-5a16-4691-83f1-332c775e78b4"
   },
   "outputs": [],
   "source": [
    "# Taking a look at a sample of the data\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discard features not used in this analysis\n",
    "df = df[['age', 'duration', 'balance', 'job', 'marital', 'education', 'y']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating dummy variables for categorical features\n",
    "df_dummies = pd.get_dummies(df, columns = ['job', 'marital', 'education', 'y'])\n",
    "df_dummies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZnQeDU9BYNf_",
    "outputId": "ed5db094-7911-44f9-f08c-0aa9ae2523a9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Taking a look at the new dummy variables\n",
    "for col in df_dummies.columns:\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLhsa8BXVbtW"
   },
   "outputs": [],
   "source": [
    "# Splitting the data into input (select features) and output\n",
    "df_dummies = df_dummies[['age', 'duration', 'balance', \n",
    "                         'job_services', 'job_management', 'job_student', 'job_retired', \n",
    "                         'marital_divorced', 'marital_married', 'marital_single', \n",
    "                         'education_primary', 'education_secondary', 'education_tertiary', \n",
    "                         'y_yes']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_dummies.drop('y_yes', axis = 1)\n",
    "y = df_dummies['y_yes']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 - Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QJdnHXbKWLlD",
    "outputId": "a64e6f38-6817-4abd-9c40-212918058233"
   },
   "outputs": [],
   "source": [
    "# Creating a random forest classifier\n",
    "# Use 100 estimators, a maximum tree depth of 5, and set the class weight as 'balanced'\n",
    "# Set the random state parameter to 123\n",
    "rf = RandomForestClassifier(n_estimators = 100, max_depth = 5, class_weight = 'balanced', random_state = 123)\n",
    "\n",
    "# Fit the model to the training data\n",
    "rf.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the feature importances from the model\n",
    "rfimp = rf.feature_importances_\n",
    "rfimp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (6, 4))\n",
    "rfimpdf = pd.DataFrame(data = {'Features': X_train.columns, 'Importances': rfimp})\n",
    "rfimpdf = rfimpdf.sort_values(by = 'Importances', ascending = False)\n",
    "sns.barplot(data = rfimpdf, x = 'Importances', y = 'Features', orient = 'h');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the above graph, ***duration*** has been determined as the most important feature by the random forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrices for the model on the training and validation data\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "fig, ax = plt.subplots(1, 2, figsize = (12, 4))\n",
    "ConfusionMatrixDisplay.from_estimator(rf, X_train, y_train, cmap = plt.cm.Blues, ax = ax[0])\n",
    "ConfusionMatrixDisplay.from_estimator(rf, X_val, y_val, cmap = plt.cm.Blues, ax = ax[1])\n",
    "ax[0].set_title('Training')\n",
    "ax[1].set_title('Validation');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining predictions on the training and testing sets\n",
    "y_pred_train = rf.predict(X_train)\n",
    "y_pred_val = rf.predict(X_val)\n",
    "\n",
    "# Compute accuracy scores\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "val_acc = accuracy_score(y_val, y_pred_val)\n",
    "\n",
    "print('Accuracy on the training data = {}'.format(train_acc))\n",
    "print('Accuracy on the validation data = {}'.format(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wN9q607KEgkM",
    "outputId": "bd059b21-4728-4598-fa92-a1c3f07c7f7c"
   },
   "outputs": [],
   "source": [
    "# Compute the ROC AUC scores for the training and the validation data\n",
    "# Obtain predicted probabilities for class '1'\n",
    "train_probabilities = rf.predict_proba(X_train)[:, 1]\n",
    "val_probabilities = rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Compute ROC AUC scores\n",
    "train_auc = roc_auc_score(y_train, train_probabilities)\n",
    "val_auc = roc_auc_score(y_val, val_probabilities)\n",
    "\n",
    "print('ROC AUC score for the training data = {}'.format(train_auc))\n",
    "print('ROC AUC score for the validation data = {}'.format(val_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 - Random Forest: Hyperparameter Tuning\n",
    "\n",
    "In this section, we will:\n",
    "- Tune the random forest model for the following hyperparameters:\n",
    "  - Number of estimators\n",
    "  - Maximum tree depth\n",
    "- Tune the random forest model for a combination of number of estimators and maximum tree depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subpart 1 - Hyperparameter Tuning: Number of Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of number of estimators to tune over\n",
    "num_estimators = np.arange(50, 550, 50)\n",
    "\n",
    "# Create and train a random forest model for each value of number of estimators\n",
    "performance_df = pd.DataFrame(data = None)\n",
    "\n",
    "# Use a for loop to loop over the different models and capture their performances\n",
    "indexcount = -1\n",
    "for current_num_estimators in num_estimators:\n",
    "    indexcount = indexcount + 1\n",
    "\n",
    "    # Create a random forest model with the current specifications\n",
    "    # Use the current number of estimators, a maximum tree depth of 5, and set the class weight as 'balanced'\n",
    "    # Set the random state parameter to 123\n",
    "    current_rf = RandomForestClassifier(n_estimators = current_num_estimators,\n",
    "                                        max_depth = 5,\n",
    "                                        class_weight = 'balanced',\n",
    "                                        random_state = 123)\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    current_rf.fit(X_train, y_train)\n",
    "\n",
    "    print('\\n Training for {} estimators is complete'.format(current_num_estimators))\n",
    "\n",
    "    # Obtain predictions\n",
    "    current_y_pred_train = current_rf.predict(X_train)\n",
    "    current_y_pred_val = current_rf.predict(X_val)\n",
    "\n",
    "    # Compute accuracy scores\n",
    "    current_train_acc = accuracy_score(y_train, current_y_pred_train)\n",
    "    current_val_acc = accuracy_score(y_val, current_y_pred_val)\n",
    "\n",
    "    # Obtain predicted probabilities for class '1'\n",
    "    current_train_probabilities = current_rf.predict_proba(X_train)[:, 1]\n",
    "    current_val_probabilities = current_rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Compute ROC AUC scores\n",
    "    current_train_auc = roc_auc_score(y_train, current_train_probabilities)\n",
    "    current_val_auc = roc_auc_score(y_val, current_val_probabilities)\n",
    "\n",
    "    tempdf = pd.DataFrame(index = [indexcount],\n",
    "                          data = {'Number of Estimators': current_num_estimators,\n",
    "                                  'Training Accuracy': current_train_acc,\n",
    "                                  'Validation Accuracy': current_val_acc,\n",
    "                                  'Training ROC AUC': current_train_auc,\n",
    "                                  'Validation ROC AUC': current_val_auc})\n",
    "\n",
    "    performance_df = pd.concat([performance_df, tempdf])\n",
    "\n",
    "performance_df.set_index('Number of Estimators')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise variation in validation accuracy scores with respect to number of estimators\n",
    "plt.figure(figsize = (10, 4))\n",
    "\n",
    "sns.lineplot(data = performance_df, x = 'Number of Estimators', y = 'Validation Accuracy', marker = 'o', markersize = 12)\n",
    "plt.title('Validation Accuracy Scores by Number of Estimators')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.xticks(num_estimators);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise variation in validation accuracy scores with respect to number of estimators\n",
    "plt.figure(figsize = (10, 4))\n",
    "\n",
    "sns.lineplot(data = performance_df, x = 'Number of Estimators', y = 'Validation ROC AUC', marker = 'o', markersize = 12)\n",
    "plt.title('Validation ROC AUC Scores by Number of Estimators')\n",
    "plt.ylabel('ROC AUC Score')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.xticks(num_estimators);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subpart 2 - Hyperparameter Tuning: Maximum Tree Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a list of maximum tree depths to tune over\n",
    "max_tree_depths = np.arange(1, 11, 1)\n",
    "\n",
    "# Create and train a random forest model for each value of maximum tree depth\n",
    "performance_df = pd.DataFrame(data = None)\n",
    "\n",
    "# Use a for loop to loop over the different models and capture their performances\n",
    "indexcount = -1\n",
    "for current_max_tree_depth in max_tree_depths:\n",
    "    indexcount = indexcount + 1\n",
    "\n",
    "    # Create a random forest model with the current specifications\n",
    "    # Use 200 estimators, the current maximum tree depth, and set the class weight as 'balanced'\n",
    "    # Set the random state parameter to 123\n",
    "    current_rf = RandomForestClassifier(n_estimators = 200,\n",
    "                                        max_depth = current_max_tree_depth,\n",
    "                                        class_weight = 'balanced',\n",
    "                                        random_state = 123)\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    current_rf.fit(X_train, y_train)\n",
    "\n",
    "    print('\\n Training for tree depth of {} is complete'.format(current_max_tree_depth))\n",
    "\n",
    "    # Obtain predictions\n",
    "    current_y_pred_train = current_rf.predict(X_train)\n",
    "    current_y_pred_val = current_rf.predict(X_val)\n",
    "\n",
    "    # Compute accuracy scores\n",
    "    current_train_acc = accuracy_score(y_train, current_y_pred_train)\n",
    "    current_val_acc = accuracy_score(y_val, current_y_pred_val)\n",
    "\n",
    "    # Obtain predicted probabilities for class '1'\n",
    "    current_train_probabilities = current_rf.predict_proba(X_train)[:, 1]\n",
    "    current_val_probabilities = current_rf.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Compute ROC AUC scores\n",
    "    current_train_auc = roc_auc_score(y_train, current_train_probabilities)\n",
    "    current_val_auc = roc_auc_score(y_val, current_val_probabilities)\n",
    "\n",
    "    tempdf = pd.DataFrame(index = [indexcount],\n",
    "                          data = {'Maximum Tree Depth': current_max_tree_depth,\n",
    "                                  'Training Accuracy': current_train_acc,\n",
    "                                  'Validation Accuracy': current_val_acc,\n",
    "                                  'Training ROC AUC': current_train_auc,\n",
    "                                  'Validation ROC AUC': current_val_auc})\n",
    "\n",
    "    performance_df = pd.concat([performance_df, tempdf])\n",
    "\n",
    "performance_df.set_index('Maximum Tree Depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise variation in validation accuracy scores with respect to the maximum tree depth\n",
    "plt.figure(figsize = (10, 4))\n",
    "\n",
    "sns.lineplot(data = performance_df, x = 'Maximum Tree Depth', y = 'Validation Accuracy', marker = 'o', markersize = 12)\n",
    "plt.title('Validation Accuracy Scores by Maximum Tree Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Maximum Tree Depth')\n",
    "plt.xticks(max_tree_depths);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualise variation in validation ROC AUC scores with respect to the maximum tree depth\n",
    "plt.figure(figsize = (10, 4))\n",
    "\n",
    "sns.lineplot(data = performance_df, x = 'Maximum Tree Depth', y = 'Validation ROC AUC', marker = 'o', markersize = 12)\n",
    "plt.title('Validation ROC AUC Scores by Maximum Tree Depth')\n",
    "plt.ylabel('ROC AUC Score')\n",
    "plt.xlabel('Maximum Tree Depth')\n",
    "plt.xticks(max_tree_depths);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subpart 3 - Hyperparameter Tuning: Combination of Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise a basic random forest classifier model\n",
    "# Set the class weight as 'balanced'\n",
    "# Set the random state parameter to 123\n",
    "base_grid_model = RandomForestClassifier(class_weight = 'balanced', random_state = 123)\n",
    "\n",
    "# Define a range of hyperparameter values to tune for and store them in a dictionary\n",
    "parameters_grid = {'n_estimators': [100, 200],\n",
    "                   'max_depth': [5, 6, 7]}\n",
    "\n",
    "# Perform a grid search using the 'GridSearchCV()' method to obtain a grid on which to fit the training data\n",
    "# Use ROC AUC score as a scoring metric\n",
    "# Use the default number of cross-validation folds\n",
    "# Set the 'verbose' parameter to 3 or more to display useful results during the process\n",
    "grid = GridSearchCV(estimator = base_grid_model,\n",
    "                    param_grid = parameters_grid,\n",
    "                    scoring = 'roc_auc',\n",
    "                    verbose = 4)\n",
    "\n",
    "# Fit the model on the training data\n",
    "grid_model = grid.fit(X_train, y_train)\n",
    "\n",
    "# Print the optimal values of 'n_estimators' and 'max_depth'\n",
    "best_n_estimators = grid_model.best_params_['n_estimators']\n",
    "best_max_depth = grid_model.best_params_['max_depth']\n",
    "best_roc_auc_score = grid_model.best_score_\n",
    "\n",
    "print('\\n The optimal model has {} estimators, each of maximum tree depth {}, and it has an ROC AUC score of {}.'.format(best_n_estimators, best_max_depth, best_roc_auc_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtyurRP9Vzci"
   },
   "source": [
    "# Part 4 - Gradient Boosted Tree\n",
    "\n",
    "We will now fit a gradient boosted tree model to the data and study the performance of the model on the training and validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RTtL1pJ8JPhx",
    "outputId": "c90cef52-b347-41fe-c0b7-a71267a27763"
   },
   "outputs": [],
   "source": [
    "# Create a gradient boosted tree classifier model\n",
    "# Use 100 estimators, a maximum tree depth of 5, a learning rate of 0.1, and set the class weight as 'balanced'\n",
    "# Set the random state parameter to 123\n",
    "gbt = LGBMClassifier(n_estimators = 100, max_depth = 5, learning_rate = 0.1, class_weight = 'balanced', random_state = 123, verbose = -1)\n",
    "\n",
    "# Fit the model to the training data\n",
    "gbt.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtaining the feature importances from the model\n",
    "gbtimp = gbt.feature_importances_\n",
    "gbtimp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 286
    },
    "id": "N3Sil6uMbvd4",
    "outputId": "c459f8e5-09a9-45f4-84f5-33e0350237eb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualising the feature importances\n",
    "plt.figure(figsize = (6, 4))\n",
    "gbtimpdf = pd.DataFrame(data = {'Features': X_train.columns, 'Importances': gbtimp})\n",
    "gbtimpdf = gbtimpdf.sort_values(by = 'Importances', ascending = False)\n",
    "sns.barplot(data = gbtimpdf, x = 'Importances', y = 'Features', orient = 'h');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the confusion matrices for the model on the training and validation data\n",
    "plt.rcParams.update({'font.size': 14})\n",
    "fig, ax = plt.subplots(1, 2, figsize = (12, 4))\n",
    "ConfusionMatrixDisplay.from_estimator(gbt, X_train, y_train, cmap = plt.cm.Blues, ax = ax[0])\n",
    "ConfusionMatrixDisplay.from_estimator(gbt, X_val, y_val, cmap = plt.cm.Blues, ax = ax[1])\n",
    "ax[0].set_title('Training')\n",
    "ax[1].set_title('Validation');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DgCJcc8DJatK"
   },
   "outputs": [],
   "source": [
    "# Compute the accuracy scores on the training and validation data\n",
    "# Obtaining predictions\n",
    "y_pred_train = gbt.predict(X_train)\n",
    "y_pred_val = gbt.predict(X_val)\n",
    "\n",
    "# Compute accuracy scores\n",
    "train_acc = accuracy_score(y_train, y_pred_train)\n",
    "val_acc = accuracy_score(y_val, y_pred_val)\n",
    "\n",
    "print('Accuracy on the training data = {}'.format(train_acc))\n",
    "print('Accuracy on the validation data = {}'.format(val_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5iQpm3DkKMIc",
    "outputId": "6b92f14f-e757-478f-970b-5d3099476061"
   },
   "outputs": [],
   "source": [
    "# Compute the ROC AUC scores for the training and validation data\n",
    "# Obtaining predicted probabilities for class '1'\n",
    "train_probabilities = gbt.predict_proba(X_train)[:, 1]\n",
    "val_probabilities = gbt.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# Compute ROC AUC scores\n",
    "train_auc = roc_auc_score(y_train, train_probabilities)\n",
    "val_auc = roc_auc_score(y_val, val_probabilities)\n",
    "\n",
    "print('ROC AUC score for the training data = {}'.format(train_auc))\n",
    "print('ROC AUC score for the validation data = {}'.format(val_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5 - Gradient Boosted Tree: Hyperparameter Tuning\n",
    "\n",
    "In this section, we will:\n",
    "- Tune the gradient boosted tree model for the following hyperparameters:\n",
    "  - Number of estimators\n",
    "  - Maximum tree depth\n",
    "  - Learning rate\n",
    "- Tune the gradient boosted tree model for a combination of the number of estimators, maximum tree depth, and learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subpart 1 - Hyperparameter Tuning: Number of Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LvSQ_S6FfS36",
    "outputId": "4d56e79b-1e8f-4aac-d914-edffd3167b4d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a list of number of estimators to tune over\n",
    "num_estimators = np.arange(50, 550, 50)\n",
    "\n",
    "# Create and train a gradient boosted tree for each value of number of estimators\n",
    "performance_df = pd.DataFrame(data = None)\n",
    "\n",
    "# Use a for loop to loop over the different models and capture their performances\n",
    "indexcount = -1\n",
    "for current_num_estimators in num_estimators:\n",
    "    indexcount = indexcount + 1\n",
    "\n",
    "    # Create a gradient boosted tree model with the current specifications\n",
    "    # Use the current number of estimators, a maximum tree depth of 5, a learning rate of 0.1, and set the class weight as 'balanced'\n",
    "    # Set the random state parameter to 123\n",
    "    current_gbt = LGBMClassifier(n_estimators = current_num_estimators,\n",
    "                                 max_depth = 5,\n",
    "                                 learning_rate = 0.1,\n",
    "                                 class_weight = 'balanced',\n",
    "                                 random_state = 123)\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    current_gbt.fit(X_train, y_train)\n",
    "\n",
    "    print('\\n Training for {} estimators is complete'.format(current_num_estimators))\n",
    "\n",
    "    # Obtain predictions\n",
    "    current_y_pred_train = current_gbt.predict(X_train)\n",
    "    current_y_pred_val = current_gbt.predict(X_val)\n",
    "\n",
    "    # Compute accuracy scores\n",
    "    current_train_acc = accuracy_score(y_train, current_y_pred_train)\n",
    "    current_val_acc = accuracy_score(y_val, current_y_pred_val)\n",
    "\n",
    "    # Obtain predicted probabilities for class '1'\n",
    "    current_train_probabilities = current_gbt.predict_proba(X_train)[:, 1]\n",
    "    current_val_probabilities = current_gbt.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Compute ROC AUC scores\n",
    "    current_train_auc = roc_auc_score(y_train, current_train_probabilities)\n",
    "    current_val_auc = roc_auc_score(y_val, current_val_probabilities)\n",
    "\n",
    "    tempdf = pd.DataFrame(index = [indexcount],\n",
    "                          data = {'Number of Estimators': current_num_estimators,\n",
    "                                  'Training Accuracy': current_train_acc,\n",
    "                                  'Validation Accuracy': current_val_acc,\n",
    "                                  'Training ROC AUC': current_train_auc,\n",
    "                                  'Validation ROC AUC': current_val_auc})\n",
    "\n",
    "    performance_df = pd.concat([performance_df, tempdf])\n",
    "\n",
    "performance_df.set_index('Number of Estimators')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "id": "D9FFRCqtj11x",
    "outputId": "9af5a459-ed2c-4c56-a527-7ffeadc5e6a4"
   },
   "outputs": [],
   "source": [
    "# Visualise variation in validation accuracy scores with respect to number of estimators\n",
    "plt.figure(figsize = (10, 4))\n",
    "\n",
    "sns.lineplot(data = performance_df, x = 'Number of Estimators', y = 'Validation Accuracy', marker = 'o', markersize = 12)\n",
    "plt.title('Validation Accuracy Scores by Number of Estimators')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.xticks(num_estimators);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "djMNmzwPshEd",
    "outputId": "5ae9afa8-5101-46c9-adad-d726c56f6bf6"
   },
   "outputs": [],
   "source": [
    "# Visualise variation in validation accuracy scores with respect to number of estimators\n",
    "plt.figure(figsize = (10, 4))\n",
    "\n",
    "sns.lineplot(data = performance_df, x = 'Number of Estimators', y = 'Validation ROC AUC', marker = 'o', markersize = 12)\n",
    "plt.title('Validation ROC AUC Scores by Number of Estimators')\n",
    "plt.ylabel('ROC AUC Score')\n",
    "plt.xlabel('Number of Estimators')\n",
    "plt.xticks(num_estimators);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subpart 2 - Hyperparameter Tuning: Maximum Tree Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LvSQ_S6FfS36",
    "outputId": "4d56e79b-1e8f-4aac-d914-edffd3167b4d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a list of maximum tree depths to tune over\n",
    "max_tree_depths = np.arange(1, 11, 1)\n",
    "\n",
    "# Create and train a gradient boosted tree model for each value of maximum tree depth\n",
    "performance_df = pd.DataFrame(data = None)\n",
    "\n",
    "# Use a for loop to loop over the different models and capture their performances\n",
    "indexcount = -1\n",
    "for current_max_tree_depth in max_tree_depths:\n",
    "    indexcount = indexcount + 1\n",
    "\n",
    "    # Create a gradient boosted tree with the current specifications\n",
    "    # Use 100 estimators, the current maximum tree depth, a learning rate of 0.1, and set the class weight as 'balanced'\n",
    "    # Set the random state parameter to 123\n",
    "    current_gbt = LGBMClassifier(n_estimators = 100,\n",
    "                                 max_depth = current_max_tree_depth,\n",
    "                                 learning_rate = 0.1,\n",
    "                                 class_weight = 'balanced',\n",
    "                                 random_state = 123)\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    current_gbt.fit(X_train, y_train)\n",
    "\n",
    "    print('\\n Training for tree depth of {} is complete'.format(current_max_tree_depth))\n",
    "\n",
    "    # Obtain predictions\n",
    "    current_y_pred_train = current_gbt.predict(X_train)\n",
    "    current_y_pred_val = current_gbt.predict(X_val)\n",
    "\n",
    "    # Compute accuracy scores\n",
    "    current_train_acc = accuracy_score(y_train, current_y_pred_train)\n",
    "    current_val_acc = accuracy_score(y_val, current_y_pred_val)\n",
    "\n",
    "    # Obtain predicted probabilities for class '1'\n",
    "    current_train_probabilities = current_gbt.predict_proba(X_train)[:, 1]\n",
    "    current_val_probabilities = current_gbt.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Compute ROC AUC scores\n",
    "    current_train_auc = roc_auc_score(y_train, current_train_probabilities)\n",
    "    current_val_auc = roc_auc_score(y_val, current_val_probabilities)\n",
    "\n",
    "    tempdf = pd.DataFrame(index = [indexcount],\n",
    "                          data = {'Maximum Tree Depth': current_max_tree_depth,\n",
    "                                  'Training Accuracy': current_train_acc,\n",
    "                                  'Validation Accuracy': current_val_acc,\n",
    "                                  'Training ROC AUC': current_train_auc,\n",
    "                                  'Validation ROC AUC': current_val_auc})\n",
    "\n",
    "    performance_df = pd.concat([performance_df, tempdf])\n",
    "\n",
    "performance_df.set_index('Maximum Tree Depth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "id": "D9FFRCqtj11x",
    "outputId": "9af5a459-ed2c-4c56-a527-7ffeadc5e6a4"
   },
   "outputs": [],
   "source": [
    "# Visualise variation in validation accuracy scores with respect to the maximum tree depth\n",
    "plt.figure(figsize = (10, 4))\n",
    "\n",
    "sns.lineplot(data = performance_df, x = 'Maximum Tree Depth', y = 'Validation Accuracy', marker = 'o', markersize = 12)\n",
    "plt.title('Validation Accuracy Scores by Maximum Tree Depth')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Maximum Tree Depth')\n",
    "plt.xticks(max_tree_depths);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "djMNmzwPshEd",
    "outputId": "5ae9afa8-5101-46c9-adad-d726c56f6bf6"
   },
   "outputs": [],
   "source": [
    "# Visualise variation in validation ROC AUC scores with respect to the maximum tree depth\n",
    "plt.figure(figsize = (10, 4))\n",
    "\n",
    "sns.lineplot(data = performance_df, x = 'Maximum Tree Depth', y = 'Validation ROC AUC', marker = 'o', markersize = 12)\n",
    "plt.title('Validation ROC AUC Scores by Maximum Tree Depth')\n",
    "plt.ylabel('ROC AUC Score')\n",
    "plt.xlabel('Maximum Tree Depth')\n",
    "plt.xticks(max_tree_depths);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subpart 3 - Hyperparameter Tuning: Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LvSQ_S6FfS36",
    "outputId": "4d56e79b-1e8f-4aac-d914-edffd3167b4d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a list of learning rates to tune over\n",
    "learning_rates = np.arange(0.02, 0.22, 0.02)\n",
    "\n",
    "# Create and train a gradient boosted tree model for each value of learning rate\n",
    "performance_df = pd.DataFrame(data = None)\n",
    "\n",
    "# Use a for loop to loop over the different models and capture their performances\n",
    "indexcount = -1\n",
    "for current_learning_rate in learning_rates:\n",
    "    indexcount = indexcount + 1\n",
    "\n",
    "    # Create a gradient boosted tree with the current specifications\n",
    "    # Use 100 estimators, a maximum tree depth of 5, the current learning rate, and set the class weight as 'balanced'\n",
    "    # Set the random state parameter to 123\n",
    "    current_gbt = LGBMClassifier(n_estimators = 100,\n",
    "                                 max_depth = 5,\n",
    "                                 learning_rate = current_learning_rate,\n",
    "                                 class_weight = 'balanced',\n",
    "                                 random_state = 123)\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    current_gbt.fit(X_train, y_train)\n",
    "\n",
    "    print('\\n Training for learning rate of {} is complete'.format(np.round(current_learning_rate, 2)))\n",
    "\n",
    "    # Obtain predictions\n",
    "    current_y_pred_train = current_gbt.predict(X_train)\n",
    "    current_y_pred_val = current_gbt.predict(X_val)\n",
    "\n",
    "    # Compute accuracy scores\n",
    "    current_train_acc = accuracy_score(y_train, current_y_pred_train)\n",
    "    current_val_acc = accuracy_score(y_val, current_y_pred_val)\n",
    "\n",
    "    # Obtain predicted probabilities for class '1'\n",
    "    current_train_probabilities = current_gbt.predict_proba(X_train)[:, 1]\n",
    "    current_val_probabilities = current_gbt.predict_proba(X_val)[:, 1]\n",
    "\n",
    "    # Compute ROC AUC scores\n",
    "    current_train_auc = roc_auc_score(y_train, current_train_probabilities)\n",
    "    current_val_auc = roc_auc_score(y_val, current_val_probabilities)\n",
    "\n",
    "    tempdf = pd.DataFrame(index = [indexcount],\n",
    "                          data = {'Learning Rate': current_learning_rate,\n",
    "                                  'Training Accuracy': current_train_acc,\n",
    "                                  'Validation Accuracy': current_val_acc,\n",
    "                                  'Training ROC AUC': current_train_auc,\n",
    "                                  'Validation ROC AUC': current_val_auc})\n",
    "\n",
    "    performance_df = pd.concat([performance_df, tempdf])\n",
    "\n",
    "performance_df.set_index('Learning Rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 518
    },
    "id": "D9FFRCqtj11x",
    "outputId": "9af5a459-ed2c-4c56-a527-7ffeadc5e6a4"
   },
   "outputs": [],
   "source": [
    "# Visualise variation in validation accuracy scores with respect to the learning rate\n",
    "plt.figure(figsize = (10, 4))\n",
    "\n",
    "sns.lineplot(data = performance_df, x = 'Learning Rate', y = 'Validation Accuracy', marker = 'o', markersize = 12)\n",
    "plt.title('Validation Accuracy Scores by Learning Rate')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.xticks(learning_rates);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "djMNmzwPshEd",
    "outputId": "5ae9afa8-5101-46c9-adad-d726c56f6bf6"
   },
   "outputs": [],
   "source": [
    "# Visualise variation in validation ROC AUC scores with respect to the learning rate\n",
    "plt.figure(figsize = (10, 4))\n",
    "\n",
    "sns.lineplot(data = performance_df, x = 'Learning Rate', y = 'Validation ROC AUC', marker = 'o', markersize = 12)\n",
    "plt.title('Validation ROC AUC Scores by Learning Rate')\n",
    "plt.ylabel('ROC AUC Score')\n",
    "plt.xlabel('Learning Rate')\n",
    "plt.xticks(learning_rates);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subpart 4 - Hyperparameter Tuning: Combinations of Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eiGgAy6o0Jjs",
    "outputId": "029fa7db-4fde-4595-e9ac-e3e7de2ba5cf"
   },
   "outputs": [],
   "source": [
    "# Initialise a basic gradient boosted classifier model\n",
    "# Set the class weight as 'balanced'\n",
    "# Set the random state parameter to 123\n",
    "base_grid_model = LGBMClassifier(class_weight = 'balanced', random_state = 123)\n",
    "\n",
    "# Define a range of hyperparameter values to tune for and store them in a dictionary\n",
    "parameters_grid = {'n_estimators': [50, 200],\n",
    "                   'max_depth': [7, 8, 9],\n",
    "                   'learning_rate': [0.06, 0.08]}\n",
    "\n",
    "# Perform a grid search using the 'GridSearchCV()' method to obtain a grid on which to fit the training data\n",
    "# Use ROC AUC score as a scoring metric\n",
    "# Use the default number of cross-validation folds\n",
    "# Set the 'verbose' parameter to 3 or more to display useful results during the process\n",
    "grid = GridSearchCV(estimator = base_grid_model,\n",
    "                    param_grid = parameters_grid,\n",
    "                    scoring = 'roc_auc',\n",
    "                    verbose = 4)\n",
    "\n",
    "# Fit the model on the training data\n",
    "grid_model = grid.fit(X_train, y_train)\n",
    "\n",
    "# Print the optimal values of 'n_estimators' and 'max_depth'\n",
    "best_n_estimators = grid_model.best_params_['n_estimators']\n",
    "best_max_depth = grid_model.best_params_['max_depth']\n",
    "best_learning_rate = grid_model.best_params_['learning_rate']\n",
    "best_roc_auc_score = grid_model.best_score_\n",
    "\n",
    "print('\\n The optimal model has {} estimators, each of maximum tree depth {}, a learning rate of {}, and it has an ROC AUC score of {}.'.format(best_n_estimators, best_max_depth, best_learning_rate, best_roc_auc_score))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Upgrad_Ensemble.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
